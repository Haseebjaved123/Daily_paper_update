# 📅 Date: 2025-09-02

## 📄 DriveQA: Passing the Driving Knowledge Test

### 👥 Authors
Maolin Wei, Wanzhou Liu, Eshed Ohn-Bar

### 🔗 Links
- **Primary**: [2508.21824v1](https://arxiv.org/abs/2508.21824v1)
- **PDF**: [Download PDF](https://arxiv.org/pdf/2508.21824v1.pdf) *(if available)*



### 🏷️ Classification
- **Primary Domain**: CV
- **Categories**: CV
- **Complexity**: Low
- **Source**: Arxiv

### 📊 Paper Statistics
- **Word Count**: 199 words
- **Sentences**: 4 sentences
- **Authors**: 3 researchers

### 🔍 Key Topics
General AI/ML

### 🛠️ Methodologies
Benchmarking, Experimentation, Dataset Creation

### 🖼️ Figure
![No Figure Available](https://img.shields.io/badge/Figure-Not_Available-lightgrey?style=for-the-badge)

### 📝 Abstract
If a Large Language Model (LLM) were to take a driving knowledge test today, would it pass? Beyond standard spatial and visual question-answering (QA) tasks on current autonomous driving benchmarks, driving knowledge tests require a complete understanding of all traffic rules, signage, and right-of-way principles. To pass this test, human drivers must discern various edge cases that rarely appear in real-world datasets. In this work, we present DriveQA, an extensive open-source text and vision-based benchmark that exhaustively covers traffic regulations and scenarios. Through our experiments using DriveQA, we show that (1) state-of-the-art LLMs and Multimodal LLMs (MLLMs) perform well on basic traffic rules but exhibit significant weaknesses in numerical reasoning and complex right-of-way scenarios, traffic sign variations, and spatial layouts, (2) fine-tuning on DriveQA improves accuracy across multiple categories, particularly in regulatory sign recognition and intersection decision-making, (3) controlled variations in DriveQA-V provide insights into model sensitivity to environmental factors such as lighting, perspective, distance, and weather conditions, and (4) pretraining on DriveQA enhances downstream driving task performance, leading to improved results on real-world datasets such as nuScenes and BDD, while also demonstrating that models can internalize text and synthetic traffic knowledge to generalize effectively across downstream QA tasks.

---
*Generated by Advanced Paper Update System - Multi-Source Intelligence*
